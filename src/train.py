import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import time
import os
from .config import Config

class ModelTrainer:
    def __init__(self, device=Config.DEVICE):
        self.config = Config
        self.device = device
    
    def train_model(self, model, train_loader, val_loader, model_name='model'):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=self.config.LEARNING_RATE, 
                             weight_decay=self.config.WEIGHT_DECAY)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
        
        train_losses = []
        val_losses = []
        train_accs = []
        val_accs = []
        
        best_acc = 0.0
        start_time = time.time()
        
        print(f"\nüéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {model_name}")
        print("=" * 60)
        
        for epoch in range(self.config.NUM_EPOCHS):
            print(f'\nEpoch {epoch+1}/{self.config.NUM_EPOCHS}')
            print('-' * 50)
            
            # Training phase
            model.train()
            running_loss = 0.0
            running_corrects = 0
            
            for inputs, labels in tqdm(train_loader, desc='Training', leave=False):
                inputs = inputs.to(self.device)
                labels = labels.to(self.device)
                
                optimizer.zero_grad()
                
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)
                
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            
            epoch_loss = running_loss / len(train_loader.dataset)
            epoch_acc = running_corrects.double() / len(train_loader.dataset)
            
            train_losses.append(epoch_loss)
            train_accs.append(epoch_acc.cpu().numpy())
            
            # Validation phase
            model.eval()
            running_loss = 0.0
            running_corrects = 0
            
            with torch.no_grad():
                for inputs, labels in tqdm(val_loader, desc='Validation', leave=False):
                    inputs = inputs.to(self.device)
                    labels = labels.to(self.device)
                    
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)
                    
                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)
            
            epoch_val_loss = running_loss / len(val_loader.dataset)
            epoch_val_acc = running_corrects.double() / len(val_loader.dataset)
            
            val_losses.append(epoch_val_loss)
            val_accs.append(epoch_val_acc.cpu().numpy())
            
            scheduler.step()
            
            print(f'‚úÖ Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            print(f'üìä Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if epoch_val_acc > best_acc:
                best_acc = epoch_val_acc
                model_path = os.path.join(self.config.MODEL_SAVE_PATH, f'best_{model_name}.pth')
                torch.save(model.state_dict(), model_path)
                print(f'üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å: {model_path}')
        
        time_elapsed = time.time() - start_time
        print(f'\nüèÅ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
        print(f'üéØ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_acc:.4f}')
        
        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accs': train_accs,
            'val_accs': val_accs,
            'best_val_accuracy': best_acc
        }
    
    def evaluate_model(self, model, test_loader, model_name="model"):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ"""
        model.eval()
        criterion = nn.CrossEntropyLoss()
        correct = 0
        total = 0
        test_loss = 0
        
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
        print("-" * 40)
        
        with torch.no_grad():
            for inputs, labels in tqdm(test_loader, desc='Testing', leave=False):
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                test_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        avg_loss = test_loss / total
        
        print(f'üìä Test Loss: {avg_loss:.4f}')
        print(f'üéØ Test Accuracy: {accuracy:.2f}%')
        print(f'‚úÖ Correct/Total: {correct}/{total}')
        
        return accuracy, avg_loss